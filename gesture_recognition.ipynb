{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand gesture recognition using Mediapipe API\n",
    "Before using\n",
    "1. [Download a pretrained model](https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/gesture_recognizer.task) and store in the root folder.\n",
    "2. Modify the `model_asset_path` as part of the `ObjectDectorOptions` class initiation to point to this location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python.vision import GestureRecognizer, GestureRecognizerOptions, GestureRecognizerResult, RunningMode\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "\n",
    "# hands, drawing, and styles are required to visualize the hand landmarks\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async output will be handled as a FIFO queue list data structure, initiate the queue\n",
    "detect_gesture_output = []\n",
    "\n",
    "# append results to the queue\n",
    "def append_gesture_result(result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    detect_gesture_output.append(result)\n",
    "\n",
    "# set up gesture recognizer options\n",
    "gesture_options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_path='gesture_recognizer.task'),\n",
    "    running_mode=RunningMode.LIVE_STREAM,\n",
    "    num_hands=2,\n",
    "    result_callback=append_gesture_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use opencv to read the detection and apply an overlay to the image\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "TEXT_COLOR = (255,255,255) #white\n",
    "\n",
    "def visualize_gesture(image: np.ndarray, detection_result: list) -> np.ndarray:\n",
    "\tresult = detection_result[0]\n",
    "\n",
    "\t# try to get the category name & hand\n",
    "\ttry:\n",
    "\t\tcategory_name = result.gestures[0][0].category_name\n",
    "\t\tcat_probability = round(result.gestures[0][0].score, 2)\n",
    "\t\thandedness = result.handedness[0][0].category_name\n",
    "\t\thand_probability = round(result.handedness[0][0].score, 2)\n",
    "\t\tcategory_result_text = f'Gesture: {category_name} ({cat_probability})'\n",
    "\t\thand_result_text = f'Hand: {handedness} ({hand_probability})'\n",
    "\t\n",
    "\t# catch error if no gesture or hand is detected\n",
    "\texcept:\n",
    "\t\tcategory_result_text = 'No gesture detected'\n",
    "\t\thand_result_text = 'No hand detected'\n",
    "\n",
    "\t# Draw text\n",
    "\tgesture_text_location = (50, 50)\n",
    "\timage = cv2.putText(img=image, text=category_result_text, org=gesture_text_location, fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "\t\t\t\t\t\tfontScale=FONT_SIZE, color=TEXT_COLOR, thickness=FONT_THICKNESS)\n",
    "\timage = cv2.putText(img=image, text=hand_result_text, org=(gesture_text_location[0], gesture_text_location[1] + 20), fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "\t\t     \t\t\tfontScale=FONT_SIZE, color=TEXT_COLOR, thickness=FONT_THICKNESS)\n",
    "\n",
    "\treturn image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that will visualize the hand landmarks from the detection result\n",
    "def visualize_hand(detection_result: list) -> landmark_pb2.NormalizedLandmarkList:\n",
    "    \"\"\"\n",
    "    Returns a NormalizedLandmarkList proto message from the first hand landmark in the detection result, this is the format needed to use the solution drawing utils\n",
    "    \"\"\"\n",
    "    result = detection_result[0]\n",
    "    hand_landmarks = result.hand_landmarks\n",
    "    hand_landmark_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    # print([landmark.x for landmark in hand_landmarks[0]])\n",
    "    tmp_list = [landmark_pb2.NormalizedLandmark(x=float(landmark.x), y=float(landmark.y), z=float(landmark.z)) for landmark in hand_landmarks[0]]\n",
    "    hand_landmark_proto.landmark.extend(tmp_list)\n",
    "    return hand_landmark_proto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GestureRecognizer` returns the following output:\n",
    "* gestures=[[Category(index=-1, score=0.6, display_name='', category_name='Thumb_Up')]], \n",
    "\n",
    "* handedness=[[Category(index=1, score=0.9, display_name='Right', category_name='Right')]], \n",
    "\n",
    "* hand_landmarks=[[NormalizedHandLandmark()]], a list of the 21 hand landmarks normalized to image\n",
    "\n",
    "* hand_world_landmarks=[[Landmark()]], a list of the 21 hand lankmarks\n",
    "\n",
    "The result is written to queue list, then popped `pop()`\n",
    "\n",
    "Figure 1. Hand Landmarks from Mediapipe\n",
    "![Hand Landmarks](https://developers.google.com/static/mediapipe/images/solutions/hand-landmarks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "W20230508 20:09:47.358242 48742 gesture_recognizer_graph.cc:128] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I20230508 20:09:47.360633 48742 hand_gesture_recognizer_graph.cc:249] Custom gesture classifier is not defined.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/robert/miniconda3/envs/mpipe/lib/python3.9/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "# initiate cv2 capture with selected live stream video device\n",
    "cap = cv2.VideoCapture(0)\n",
    "# the live stream recognizer requires a 'time stamp', initiate a counter to act as timestamp\n",
    "start = 0\n",
    "\n",
    "# Initiate the recognizer\n",
    "with GestureRecognizer.create_from_options(gesture_options) as recognizer:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert color\n",
    "    image = cv2.flip(image, 1) # flip the image for proper handedness detection\n",
    "    start += 1 # increment the frame\n",
    "    image.flags.writeable = True\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "\n",
    "    recognizer.recognize_async(mp_image, start) # run gesture recognition\n",
    "\n",
    "    # handle the FIFO detection queue, once there are more than 1 result, pop the first one\n",
    "    if len(detect_gesture_output) > 1:\n",
    "      detect_gesture_output.pop(0)\n",
    "      result_image = visualize_gesture(image, detect_gesture_output)\n",
    "    elif len(detect_gesture_output) == 1:\n",
    "      result_image = visualize_gesture(image, detect_gesture_output)\n",
    "    else:\n",
    "      result_image = image\n",
    "\n",
    "    return_image = cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # try to draw the hand landmarks, uses the same queue as the gesture recognition\n",
    "    try:\n",
    "      hand = visualize_hand(detect_gesture_output)\n",
    "      mp_drawing.draw_landmarks(\n",
    "        image=return_image,\n",
    "        landmark_list=hand,\n",
    "        connections=mp_hands.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "        connection_drawing_spec=mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2))\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "    cv2.imshow('Gesture Recognizer', return_image)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "      break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
