{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection in python using Mediapipe API\n",
    "Before beginning\n",
    "1. [Download a pre-trained model](https://developers.google.com/mediapipe/solutions/vision/object_detector#efficientdet-lite0_model_recommended) and store it in the root folder.\n",
    "2. Modify the `model_asset_path` as part of the `ObjectDectorOptions` class initiation to point to this location.\n",
    "3. Modify the `max_results` kwarg for the `ObjectDetectorOptions` to change the number of return detections, up to 7 are coded based on the color list `TEXT_COLOR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python.components.containers.detections import DetectionResult\n",
    "from mediapipe.tasks.python.vision import ObjectDetector, ObjectDetectorOptions, RunningMode\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "DetectionResult = DetectionResult\n",
    "\n",
    "# async output will be handled as a FIFO queue list data structure, initiate the queue\n",
    "detect_output = []\n",
    "\n",
    "# append results to the queue\n",
    "def append_result(result: DetectionResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    detect_output.append(result)\n",
    "\n",
    "\n",
    "options = ObjectDetectorOptions(\n",
    "    base_options=BaseOptions(model_asset_path='efficientdet.tflite'),\n",
    "    running_mode=RunningMode.LIVE_STREAM,\n",
    "    max_results=2,\n",
    "    result_callback=append_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN = 10  # pixels\n",
    "ROW_SIZE = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "# red, green, blue, cyan, magenta, yellow, black, white\n",
    "TEXT_COLOR = [(255, 0, 0), (0,255,0), (0,0,255), (0,255,255), (255,0,255), (255,255,0), (0,0,0), (255,255,255)]\n",
    "\n",
    "def visualize(image, detection_result) -> np.ndarray:\n",
    "\t\"\"\"Draws bounding boxes on the input image and return it.\n",
    "\tArgs:\n",
    "\timage: The input RGB image.\n",
    "\tdetection_result: The list of all \"Detection\" entities to be visualize.\n",
    "\tReturns:\n",
    "\tImage with bounding boxes.\n",
    "\t\"\"\"\n",
    "\tfor detection in detection_result.detections:\n",
    "\t\tindex = detection_result.detections.index(detection)\n",
    "\t\t# Draw bounding_box\n",
    "\t\tbbox = detection.bounding_box\n",
    "\t\tstart_point = bbox.origin_x, bbox.origin_y\n",
    "\t\tend_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "\t\tcv2.rectangle(image, start_point, end_point, TEXT_COLOR[index], 3)\n",
    "\n",
    "\t\t# Draw label and score\n",
    "\t\tcategory = detection.categories[0]\n",
    "\t\tcategory_name = category.category_name\n",
    "\t\tprobability = round(category.score, 2)\n",
    "\t\tresult_text = category_name + ' (' + str(probability) + ')'\n",
    "\t\ttext_location = (MARGIN + bbox.origin_x,\n",
    "\t\t\t\t\t\tMARGIN + ROW_SIZE + bbox.origin_y)\n",
    "\t\tcv2.putText(image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "\t\t\t\t\tFONT_SIZE, TEXT_COLOR[index], FONT_THICKNESS)\n",
    "\n",
    "\t\n",
    "\treturn image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/robert/miniconda3/envs/mpipe/lib/python3.9/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "start = 0\n",
    "with ObjectDetector.create_from_options(options) as detector:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    start += 1\n",
    "    image.flags.writeable = True\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "    detector.detect_async(mp_image, start) #result_callback prints a result\n",
    "    # handle the queue, if there are more than 1 result, pop the first one\n",
    "    if len(detect_output) > 1:\n",
    "      detect_output.pop(0)\n",
    "      image = visualize(image, detect_output[0])\n",
    "    elif len(detect_output) == 1:\n",
    "      image = visualize(image, detect_output[0])\n",
    "\n",
    "    return_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('Mediapipe FaceMesh', return_image)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "      break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
